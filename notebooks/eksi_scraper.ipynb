{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# !pip3 install beautifulsoup4\n",
    "# !pip3 install selenium\n",
    "# !pip3 install pymongo==3.11.2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import environ\n",
    "\n",
    "env = environ.Env()\n",
    "env.read_env(env.str('ENV_PATH', '.env'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mongo_cli_username = os.environ.get('MONGO_CLI_USERNAME')\n",
    "mongo_cli_password = os.environ.get('MONGO_CLI_PASSWORD')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# target url\n",
    "url = \"https://eksisozluk.com/\"\n",
    "\n",
    "# input location\n",
    "input_location = 'data/input/keywords.txt'\n",
    "\n",
    "# output location\n",
    "output_location = 'data/output/eksi.json'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "client = MongoClient(\"mongodb+srv://{}:{}@cluster0.plop5.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\".format(mongo_cli_username, mongo_cli_password))\n",
    "db = client['healdash']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# keywords list\n",
    "keywords = []\n",
    "\n",
    "with open(input_location) as my_file:\n",
    "    for line in my_file:\n",
    "        keywords.append(line.replace(\"\\n\", \"\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# show keywords\n",
    "keywords"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['hazımsızlık', 'gaz']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# class structure\n",
    "class Eksi:\n",
    "    def __init__(self, url: str) -> None:\n",
    "        self.url = url\n",
    "        \n",
    "        # init the browser\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--ignore-certificate-errors')\n",
    "        options.add_argument('--incognito')\n",
    "        options.add_argument('--headless')\n",
    "\n",
    "        driver = webdriver.Chrome() # initialize the driver\n",
    "        driver.get(self.url) # go to the url\n",
    "        self.driver = driver\n",
    "        \n",
    "    def search_keyword(self, keyword: str) -> None:\n",
    "        search_input = self.driver.find_element_by_id(\"search-textbox\")\n",
    "        search_input.clear()\n",
    "        search_input.send_keys(keyword)\n",
    "        search_input.submit()\n",
    "        time.sleep(0.5) # small delay before getting the page source\n",
    "        \n",
    "    def compile_page_source(self) -> object:\n",
    "        page_source = self.driver.page_source # get the page source\n",
    "        soup = BeautifulSoup(page_source.encode('utf-8','ignore')) # compile it with bs4\n",
    "        try:\n",
    "            self.max_pages = int(soup.find('div', {\"class\": \"pager\"})['data-pagecount'])\n",
    "        except:\n",
    "            self.max_pages = 1\n",
    "        self.keyword_scape_time = self.max_pages * 0.5\n",
    "        self.page_source = soup\n",
    "        self.keyword_exists() # detect if the keyword exists\n",
    "        return self\n",
    "    \n",
    "    def next_page(self, page_number: int) -> None:\n",
    "        current_url = self.driver.current_url \n",
    "        current_url = current_url[:current_url.rfind(\"?\")+1] # remove all url variables \n",
    "        \n",
    "        # if there are not parameters in the existing url\n",
    "        if not current_url:\n",
    "            current_url = self.driver.current_url + \"?\"\n",
    "            \n",
    "        current_url = current_url + ('p={}'.format(page_number))\n",
    "        self.driver.get(current_url)\n",
    "        \n",
    "    def clean_entry(self, entry: str) -> str: \n",
    "        return (\n",
    "            entry\n",
    "            .replace(\"\\n\", \"\") # remove new lines\n",
    "            .replace(\"\\'\", \"'\") # fix apostrophe\n",
    "            .strip() # remove spaces\n",
    "        )\n",
    "    \n",
    "    def keyword_exists(self) -> bool:\n",
    "        all_authors = self.page_source.find_all('a', {\"class\": \"entry-author\"}) # get all authors\n",
    "        if len(all_authors):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def scrape_data(self, keyword: str) -> None:\n",
    "        all_entries = self.page_source.find_all('div', {\"class\": \"content\"}) # get all entries\n",
    "        all_dates = self.page_source.find_all('a', {\"class\": \"entry-date\"}) # get all dates\n",
    "        all_authors = self.page_source.find_all('a', {\"class\": \"entry-author\"}) # get all authors\n",
    "        \n",
    "        for entry, date, author in zip(all_entries, all_dates, all_authors):\n",
    "            self.keyword_dict[keyword].append({\n",
    "                \"date\": date.text, \n",
    "                \"author\": author.text, \n",
    "                \"entry\": self.clean_entry(entry.text)\n",
    "            })\n",
    "          \n",
    "    def scrape_all_pages(self, keyword_list: list) -> None:\n",
    "        \n",
    "        # reset keywords dict\n",
    "        self.keyword_dict = defaultdict(list)\n",
    "        \n",
    "        for keyword in keyword_list:\n",
    "            self.search_keyword(keyword)\n",
    "            self.compile_page_source() # compile for the first time\n",
    "            \n",
    "            if self.keyword_exists:\n",
    "                print(\"{} - scraping time: {} seconds\".format(keyword, self.keyword_scape_time)) # print scraping time for the keyword\n",
    "\n",
    "                for i in range(1, self.max_pages + 1):\n",
    "                    self.next_page(i)\n",
    "                    self.compile_page_source().scrape_data(keyword)\n",
    "                    \n",
    "                # add data to mongodb\n",
    "                db.eksi_entries.update_many({\"keyword\": keyword}, {\"$set\": {\"objects\": self.keyword_dict[keyword]}}, upsert=True)\n",
    "            else:\n",
    "                print(\"No results for {}\".format(keyword))\n",
    "                \n",
    "    def get_json_output(self, output_location: str) -> None:\n",
    "        \n",
    "        # dump the json file\n",
    "        json_object = json.dumps(self.keyword_dict, ensure_ascii=False).encode('utf-8','ignore').decode() \n",
    "        \n",
    "        # get the output\n",
    "        with open(output_location, 'w+', encoding='utf-8') as f: \n",
    "            json.dump(json_object, f, ensure_ascii=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# initialize the object\n",
    "eksi = Eksi(url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# scrape the data\n",
    "eksi.scrape_all_pages(keywords)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hazımsızlık - scraping time: 2.5 seconds\n",
      "gaz - scraping time: 2.5 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# get json output\n",
    "eksi.get_json_output(output_location)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# example\n",
    "# eksi.keyword_dict"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target url\n",
    "url = \"https://eksisozluk.com/\"\n",
    "\n",
    "# keywords list\n",
    "keywords = [\"gaz\", \"hazımsızlık\", \"kabızlık\"]\n",
    "\n",
    "# output location\n",
    "output_location = 'data/output/data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class structure\n",
    "class Eksi:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        \n",
    "        # init the browser\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--ignore-certificate-errors')\n",
    "        options.add_argument('--incognito')\n",
    "        options.add_argument('--headless')\n",
    "\n",
    "        driver = webdriver.Chrome() # initialize the driver\n",
    "        driver.get(self.url) # go to the url\n",
    "        self.driver = driver\n",
    "        \n",
    "    def search_keyword(self, keyword):\n",
    "        element = self.driver.find_element_by_id(\"search-textbox\")\n",
    "        element.send_keys(keyword)\n",
    "        element.submit()\n",
    "        time.sleep(1) # small delay before getting the page source\n",
    "        \n",
    "    def compile_page_source(self):\n",
    "        page_source = self.driver.page_source # get the page source\n",
    "        soup = BeautifulSoup(page_source.encode('utf-8','ignore')) # compile it with bs4\n",
    "        self.max_pages = int(soup.find('div', {\"class\": \"pager\"})['data-pagecount'])\n",
    "        self.page_source = soup\n",
    "        return self\n",
    "    \n",
    "    def next_page(self, page_number):\n",
    "        current_url = self.driver.current_url \n",
    "        current_url = current_url[:current_url.rfind(\"?\")+1] # remove all url variables \n",
    "        \n",
    "        # if there are not additional parameter in the existing url\n",
    "        if not current_url:\n",
    "            current_url = self.driver.current_url + \"?\"\n",
    "            \n",
    "        current_url = current_url + ('p={}'.format(page_number))\n",
    "        self.driver.get(current_url)\n",
    "        \n",
    "    def clean_entry(self, entry):\n",
    "        \n",
    "        return (\n",
    "            entry\n",
    "            .replace(\"\\n\", \"\") # remove new lines\n",
    "            .replace(\"\\'\", \"'\") # fix apostrophe\n",
    "            .strip() # remove spaces\n",
    "        )\n",
    "        \n",
    "    def scrape_data(self, keyword: str):\n",
    "        \n",
    "        all_entries = self.page_source.find_all('div', {\"class\": \"content\"}) # get all entries\n",
    "        all_dates = self.page_source.find_all('a', {\"class\": \"entry-date\"}) # get all dates\n",
    "        all_authors = self.page_source.find_all('a', {\"class\": \"entry-author\"}) # get all authors\n",
    "        for entry, date, author in zip(all_entries, all_dates, all_authors):\n",
    "            self.keyword_dict[keyword].append((date.text, author.text, self.clean_entry(entry.text)))\n",
    "        \n",
    "    def scrape_all_pages(self, keyword_list: list):\n",
    "        \n",
    "        # reset keywords dict\n",
    "        self.keyword_dict = defaultdict(list)\n",
    "        \n",
    "        for keyword in keyword_list:\n",
    "            self.search_keyword(keyword)\n",
    "            self.compile_page_source()\n",
    "\n",
    "            for i in range(1, self.max_pages + 1):\n",
    "                self.next_page(i)\n",
    "                self.compile_page_source().scrape_data(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the object\n",
    "eksi = Eksi(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the data\n",
    "eksi.scrape_all_pages(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eksi.keyword_dict[keywords[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the json file\n",
    "json_object = json.dumps(eksi.keyword_dict, ensure_ascii=False).encode('utf-8','ignore').decode() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output\n",
    "with open(output_location, 'w+', encoding='utf-8') as f: \n",
    "    json.dump(json_object, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
